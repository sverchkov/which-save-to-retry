\documentclass{article}

\usepackage{amsmath,amsfonts}

\title{Best strategy for correcting a single detection in a stealth level playthrough}

\begin{document}

\maketitle

\section{Problem}
Knowing that the player was detected once in a playthrough of a stealth level, without knowing where the detection happened, to which save is it best to go back to attempt to correct the detection?

\section{Formulation}
W.l.o.g. assume the level starts at time 0 and ends at time 1.
For simplicity assume that we have infinite saves at every (real) timepoint between 0 and 1.
For simplicity assume that the correcting playthrough will be perfect.
Furthermore, if cost is player time, the cost of playing from time $t \quad (0\leq t \leq1)$ is $(1-t)$.
Another strong assumption here is that it always takes the same amount of time to play through any given section.
Assume that the detection happened at some random time $T$ where $T \sim U(0,1)$ (uniformly random over the interval).

\section{Strategy specification}
Since after each attempt we either finally succeed or must select a single next timepoint to test from, every strategy can be uniquely characterized by an infinite sequence of timepoints $(t_1, t_2, t_3, \ldots )$, $0 < \ldots < t_3 < t_2 < t_1 < 1$.

\section{Unrolling the expected value}
The expected cost of a strategy $(t_1, t_2, t_3, \ldots )$ is then:
\[
 \underbrace{(1 - t_1)}_{\text{cost}} \cdot \underbrace{(1 - t_1)}_{\text{probability}} +
 \left(
  \underbrace{(1-t_1)}_{\text{cost of failing}} +
  \underbrace{(1 - t_2)}_{\text{cost}} \cdot \underbrace{\frac{t_1 - t_2}{t_1}}_{\text{probability}} +
  \left( (1-t_2) + (1-t_3) \cdot \frac{t_2-t_3}{t_2} + (\ldots)\cdot \frac{t_3}{t_2}
  \right) \cdot \frac{t_2}{t_1}
 \right) \cdot \underbrace{t_1}_{\text{probability}}
\]
\[
= \underbrace{(1-t_1)}_{\text{cost of trying 1st}} + t_1 \left( (1-t_2) + \frac{t_2}{t_1} \left( (1-t_3) + \frac{t_3}{t_2}(\cdots) \right) \right)
\]
Let $t_0=1$ (that makes notation consistent).
\[
= \sum_{i=1}^\infty t_{i-1}(1-t_i)
\]

\section{The finite problem}
Suppose that we are limited to $k$ saves, where we replay the entire level if we fail after all $k$ saves.
For that case we get:
\begin{equation}\label{eq:finite}
C = \sum_{i=1}^k t_{i-1} (1 - {t_i}) + t_k
\end{equation}

\section{The optimal subset case.}

Clearly, in a realistic setting we only have $n$ saves to choose from, and the problem becomes a question of deciding how many saves to pick and which ones to pick.
Approach this problem by comparing the costs of a finite strategy with the cost of that strategy with save $j$ removed.
Let the expected cost of the ``full'' strategy be $C$, given by~\eqref{eq:finite}, and the cost of the strategy without $j$ be $C_{-j}$.

\paragraph{The case of $1<j<k$}
\begin{align*}
 C - C_{-j} &= t_{j-1} (1-t_j) + t_j(1-t_{j+1}) - t_{j-1} ( 1-t_{j+1} ) \\
 &= t_{j-1} - t_j t_{j-1} + t_j -t_j t_{j+1} - t_{j-1} + t_{j-1} t_{j+1} \\
 &= t_j (1-t_{j-1}-t_{j+1} ) + t_{j-1} t_{j+1}
\end{align*}
Adding $t_j$ is only beneficial when it deceases the cost, i.e.\ $C- C_{-j} < 0$, i.e.:
\[
 t_j > \frac{t_{j-1} t_{j+1}}{ t_{j-1} + t_{j+1} - 1 } \qquad \text{s.t. } t_{j-1} + t_{j+1} - 1 > 0 
\]
\paragraph{The case of $j=1$}
\begin{align*}
 C - C_{-1} &=  (1-t_1)+t_1(1-t_2) - (1-t_2) \\
 &= (1-t_1) + (t_1 - 1) ( 1 - t_2) \\
 &= (1-t_1) t_2
\end{align*}
This is always greater than 0.
\paragraph{The case of $j=k$}
\[
 C_{-k} = \sum_{i=1}^{k-1} t_{i-1} (1 - {t_i}) + t_{k-1}
\]
\[
C - C_{-k} = t_k + t_{k-1}(1-t_{k}) -t_{k-1} = t_k (1-t_{k-1})
\]
This is also always greater than 0.

Having computed these costs of adjusting a strategy by one term, once can construct a dynamic programming algorithm to find the best strategy given a finite set of saves.

\end{document}